{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61c7bba-7135-4cca-ae70-012e7a866a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tom_r\\anaconda3\\envs\\LLM_Projects\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import huggingface\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c289a06-9d70-4e47-9e85-2f07fd9dc731",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226e1a69-4cd0-49be-97c6-fc6c69d297f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"abisee/cnn_dailymail\", \"1.0.0\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d0484-d0e1-4630-8836-519844972146",
   "metadata": {},
   "source": [
    "#### Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2430be-7386-47df-99d2-a98fa73579d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT ARTICLE:\n",
      "(CNN)According to an outside review by Columbia Journalism School professors, \"(a)n institutional failure at Rolling Stone resulted in a deeply flawed article about a purported gang rape at the University of Virginia.\" The Columbia team concluded that \"The failure encompassed reporting, editing, editorial supervision and fact-checking.\" Hardly a ringing endorsement of the editorial process at the publication. The magazine's managing editor, Will Dana, wrote, \"We would like to apologize to our readers and to all of those who were damaged by our story and the ensuing fallout, including members of the Phi Kappa Psi fraternity and UVA administrators and students.\" Brian Stelter: Fraternity to 'pursue all available legal action' The next question is: . Can UVA, Phi Kappa Psi or any of the other fraternities on campus sue for defamation? The Virginia Supreme Court said in Jordan v. Kollman that \"the elements of libel are (1) publication of (2) an actionable statement with (3) the requisite intent.\" \"Actionable\" means the statement must be both false and defamatory. Of course, the law of defamation must be balanced against the freedom of speech protected under not only the First Amendment to the United States Constitution, but also the Virginia Constitution. True statements cannot be defamatory. Neither can pure statements of opinion, because they theoretically cannot be either true or false. But the Rolling Stone article certainly purported to be fact, and it apparently is not exactly what the law considers \"true.\" The individual members of the fraternity will likely be considered private individuals, and not public figures; the latter have a harder time proving defamation. A private person suing for defamation must establish that the defendant has published a false factual statement that is about the person and that it also harms the person's reputation. The private plaintiff also must show that the defendant knew that the statement was false, or believed it was true but lacked a reasonable basis, or acted negligently in checking the facts. At first blush, that sounds like it fits perfectly, right? The Columbia report may go a long way toward establishing at least a modicum of the required intent. But that's only half the battle. There are strict rules about who can be a plaintiff in a defamation action like this. The identity of the aspiring plaintiff matters. First, let's eliminate UVA. The university is a public university, and therefore it is a governmental entity. The Supreme Court has been clear on the issue of libelous statements about the government: The government cannot sue for defamation. There is no such cause of action in American jurisprudence. Now the fraternities, starting with Phi Kappa Psi. A fraternity is not an individual, but a group. A plaintiff in a defamation case must show that the statements were \"of or concerning\" the plaintiff. It sounds obvious, but if you're going to say a statement hurt you, you have to prove the statement actually was about you to begin with. When the statements are about a group without naming an individual, it's hard to say the statement is \"concerning\" the individual -- and groups generally cannot sue.  For example, you can be sued if you call a specific lawyer a thief, but that same person cannot sue you if you simply call all lawyers thieves. Defamatory statements about a group are therefore not actionable by the group's individual members, for the most part. Like all rules, however, there are exceptions. If the defamatory language is about \"a comparatively small group of persons and the defamatory part is easily imputed against all members of the small group, an individual member may sue.\" If I said, \"The 1980 Philadelphia Phillies infielders were a bunch of criminals\" (they weren't),  the individual players could sue, because that mean statement is clearly about certain persons -- if I said that -- which I didn't. Phi Kappa Psi would likely argue that the \"small group\" exception fits it perfectly: Even if the individual members were not identified by name, the defamatory story has been imputed directly to individual members, who have suffered by their association with the group. On the other hand, Rolling Stone's lawyers would likely argue that the group is so large and fluid (after all, the membership changes somewhat every year), that even though the fraternity's reputation is tarnished, the members have suffered no individualized injury. As for the other fraternities on campus but not implicated in the story, that's likely a group that moves from the small category to large, and the members of Greek life generally will have a harder time bringing a lawsuit. Lawyers will tell you that a libel suit is one of those things that citizens often threaten each other with on Facebook, but that such cases are rarely actually filed. That's because a plaintiff usually has to show some kind of financial harm. So if your Aunt Edna calls you a loser on Twitter, you're going to have to spend money on an expert to explain to a jury how that actually damaged you financially. And since most of the people who waste time threatening each other with defamation suits  on Facebook live in their moms' basements and are \"between jobs,\" these are not the kind of people who have money or reputation to damage in the first place. The UVA situation is not your run-of-the-mill defamation case. The university won't be able to sue, but if the members of the fraternity can get past some of the preliminary hurdles of a defamation claim, and they can make a tangible case for damages, then this could be one of those rare successful defamation cases.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN HIGHLIGHTS:\n",
      "An outside review found that a Rolling Stone article about campus rape was \"deeply flawed\" Danny Cevallos says that there are obstacles to a successful libel case, should one be filed .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT ARTICLE:\n",
      "(CNN)Anthony Ray Hinton is thankful to be free after nearly 30 years on Alabama's death row for murders he says he didn't commit. And incredulous that it took so long. Hinton, 58, looked up, took in the sunshine and thanked God and his lawyers Friday morning outside the county jail in Birmingham, minutes after taking his first steps as a free man since 1985. He spoke of unjustly losing three decades of his life, under fear of execution, for something he didn't do. \"All they had to do was to test the gun, but when you think you're high and mighty and you're above the law, you don't have to answer to nobody,\" Hinton told reporters. \"But I've got news for you -- everybody that played a part in sending me to death row, you will answer to God.\" Jefferson County Circuit Court Judge Laura Petro had ordered Hinton released after granting the state's motion to dismiss charges against him. Hinton was convicted of murder in the 1985 deaths of two Birmingham-area, fast-food restaurant managers, John Davidson and Thomas Wayne Vason. But a new trial was ordered in 2014 after firearms experts testified 12 years earlier that the revolver Hinton was said to have used in the crimes could not be matched to evidence in either case, and the two killings couldn't be linked to each other. \"Death Row Stories\": Hard questions about the U.S. capital punishment system . The state then declined to re-prosecute the case. Hinton was 29 at the time of the killings and had always maintained his innocence, said the Equal Justice Initiative, a group that helped win his release. \"Race, poverty, inadequate legal assistance, and prosecutorial indifference to innocence conspired to create a textbook example of injustice,\" Bryan Stevenson, the group's executive director and Hinton's lead attorney, said of his African-American client. \"I can't think of a case that more urgently dramatizes the need for reform than what has happened to Anthony Ray Hinton.\" Stevenson said the \"refusal of state prosecutors to re-examine this case despite persuasive and reliable evidence of innocence is disappointing and troubling.\" Amnesty report: Executions down but death sentences on the rise . Dressed in a dark suit and blue shirt, Hinton praised God for his release, saying he was sent \"not just a lawyer, but the best lawyers.\" He said he will continue to pray for the families of the murder victims. Both he and those families have suffered a miscarriage of justice, he said. \"For all of us that say that we believe in justice, this is the case to start showing, because I shouldn't have (sat) on death row for 30 years,\" he said. Woman who spent 22 years on death row has case tossed . Hinton was accompanied Friday by two of his sisters, one of whom still lives in the Birmingham area. Other siblings will fly to the area to see him soon, Stevenson said. His mother, with whom he lived at the time of his arrest, is no longer living, according to the lawyer. Hinton planned to spend at least this weekend at the home of a close friend. He will meet with his attorneys Monday to start planning for his immediate needs, such as obtaining identification and getting a health checkup, Stevenson said. The plan now is to spend a few weeks to get oriented with freedom and \"sort out what he wants to do,\" Stevenson said.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN HIGHLIGHTS:\n",
      "Anthony Ray Hinton goes free Friday, decades after conviction for two murders . Court ordered new trial in 2014, years after gun experts testified on his behalf . Prosecution moved to dismiss charges this year .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example  3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT ARTICLE:\n",
      "(CNN)A judge this week sentenced a former TSA agent to six months in jail for secretly videotaping a female co-worker while she was in the bathroom, prosecutors said. During the investigation, detectives with the Metro Nashville Police Department in Tennessee also found that the agent, 33-year-old Daniel Boykin, entered the woman's home multiple times, where he took videos, photos and other data. Police found more than 90 videos and 1,500 photos of the victim on Boykin's phone and computer. The victim filed a complaint after seeing images of herself on his phone last year. Boykin plead guilty to unlawful photography, aggravated burglary and violation of the computer act, the Nashville District Attorney's Office said. Police said the incident happened in a TSA-only restroom, and that there was no evidence public restrooms were targeted. A TSA official tells CNN that Boykin worked in an administrative capacity and didn't engage in public security screening. Assistant District Attorney Amy Hunter said this case was one of the worst invasion of privacy cases she's seen. \"We are thankful that the sentence includes periodic confinement so that the sentence will hopefully make an impression on this defendant and others,\" Hunter said in a statement. The judge, Randall Wyatt, on Friday called the invasion of privacy \"egregious.\" His sentence also includes five and a half years of probation, which will include GPS monitoring. Boykin was terminated last year when the investigation began. \"TSA holds its employees to the highest ethical standards and has zero tolerance for misconduct in the workplace,\" TSA's Ross Feinstein said in a statement.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN HIGHLIGHTS:\n",
      "Former TSA agent Daniel Boykin, 33, videotaped his female co-worker in the restroom, authorities say . Authorities say they found 90 videos and 1,500 photos of the victim on Boykin's phone and computer . Boykin worked in an administrative capacity and didn't do public security screenings, TSA official says .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example  4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT ARTICLE:\n",
      "(CNN)Former New England Patriots star Aaron Hernandez will need to keep his lawyers even after being convicted of murder and other charges in the death of Odin Lloyd. The 25-year-old potentially faces three more trials -- one criminal and two civil actions. Next up is another murder trial in which he is accused of killing two men and wounding another person near a Boston nightclub in July 2012. Prosecutors have said Hernandez fatally shot Daniel de Abreu and Safiro Furtado when he fired into their 2003 BMW.  Another passenger was wounded and two others were uninjured. Hernandez pleaded not guilty at his arraignment. The trial was originally slated for May 28, but Jake Wark, spokesman for the Suffolk County District Attorney's Office, said Wednesday the trial had been postponed and no new date had been set. \"We expect to select a new court date in the coming days and then set the amended trial track. The Suffolk indictments allege two counts of first-degree murder for the July 16, 2012, shooting deaths of Daniel de Abreu and Safiro Furtado in Boston's South End; three counts of armed assault with intent to murder and one count of assault and battery by means of a dangerous weapon for shots fired at three surviving victims; and one count of unlawful possession of a firearm,\" he said. The families of de Abreu and Furtado filed civil suits against Hernandez, and a judge froze his $5 million in assets, pending the outcome of the double-murder trial. The freeze includes the disputed $3.3 million signing bonus payment Hernandez claims he is owed by the New England Patriots. Hernandez is also being sued by a man who claims Hernandez shot him while they were in a limousine in Miami in February 2013. Alexander Bradley claims the then-New England Patriot tight end wounded him after the two got into a fight at a Miami strip club. In a lawsuit filed four months later, Bradley said Hernandez fired at him during a limo ride after leaving the club and that Hernandez intentionally \"possessed a gun which he was not legally licensed to have.\" Hernandez's lawyers have argued he couldn't defend himself properly while on trial in Massachusetts. There was no criminal charge in the case. And then there is the grievance over unpaid bonus money filed by the NFL players union on behalf of Hernandez, who signed a contract in 2012 that potentially was worth more than $40 million. If the grievance is heard by the league, Hernandez will be represented by the the National Football League Players' Association. Who was Odin Lloyd? CNN's Lawrence Crook contributed to this report.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN HIGHLIGHTS:\n",
      "Aaron Hernandez has been found guilty in Odin Lloyd's death, but his troubles are not over . He also faces murder charges in Suffolk County, Massachusetts, but trial was postponed . In addition, Hernandez will face two civil lawsuits; one is in relation to Suffolk County case .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring some examples from the dataset\n",
    "\n",
    "example_indices = [50, 100, 154, 200]\n",
    "\n",
    "\n",
    "dash_line = 100 * '-'\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT ARTICLE:')\n",
    "    print(dataset['test'][index]['article'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN HIGHLIGHTS:')\n",
    "    print(dataset['test'][index]['highlights'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0c088-a8e0-4e7b-ba82-9c3e7676dfb3",
   "metadata": {},
   "source": [
    "These examples show that the news articles get summurised by in a fairly succinct fashion to give just the main highlights for each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80f39c-6c84-4052-929e-576a76d78f4f",
   "metadata": {},
   "source": [
    "#### Pre-process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319a9b1-5e39-4f5e-a00d-bd56dcae9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb671c0-b12e-478d-ae9c-0d06bd559f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(batch_size, data, category):\n",
    "    '''\n",
    "    Function to handle batch tokenizing each part of dataset\n",
    "    batch_size(int): number of articles to be tokenized at once (reduced to help with compute)\n",
    "    data (string): specifying train, validation or test subsets of the dataset\n",
    "    category (string): specifying 'article' or 'highlights'\n",
    "    \n",
    "    '''\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset[data][category]), batch_size)):\n",
    "        batch_training_articles = dataset[data][category][i:i+batch_size]\n",
    "        tokenized_batch = tokenizer(batch_training_articles, max_length=512, padding = True, truncation = True, return_tensors = 'pt')\n",
    "        input_ids.extend(tokenized_batch['input_ids'])\n",
    "        attention_masks.extend(tokenized_batch['attention_mask'])\n",
    "                        \n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24feca4-b6c8-499a-b0bb-fc478a37f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially I am going to not use the train subset to reduce the dataset size and the time spent tokenizing and training. I will include this set in the future\n",
    "\n",
    "\n",
    "# Train\n",
    "# print(\"Tokenizing Train:Article\")\n",
    "# train_tokenized_article = tokenize_dataset(1000, 'train', 'article')\n",
    "\n",
    "# print(\"Tokenizing Train:Highlight\")\n",
    "# train_tokenized_highlight = tokenize_dataset(1000, 'train', 'highlight')\n",
    "\n",
    "\n",
    "# Validation\n",
    "print(\"Tokenizing Validation:Article\")\n",
    "validation_tokenized_article_ids, validation_tokenized_article_masks  = tokenize_dataset(1000, 'validation', 'article')\n",
    "\n",
    "print(\"Tokenizing Validation:Highlight\")\n",
    "validation_tokenized_highlight_ids, validation_tokenized_highlight_masks = tokenize_dataset(1000, 'validation', 'highlights')\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Tokenizing Test:Article\")\n",
    "test_tokenized_article_ids, test_tokenized_article_masks = tokenize_dataset(1000, 'test', 'article')\n",
    "\n",
    "print(\"Tokenizing Test:Highlight\")\n",
    "test_tokenized_highlight_ids, test_tokenized_highlight_masks = tokenize_dataset(1000, 'test', 'highlights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b506e4d-1d8f-4283-92e4-420248e57be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized data\n",
    "def save_tokenized_data(tokenized_data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(tokenized_data, f)\n",
    "\n",
    "# Load tokenized data\n",
    "def load_tokenized_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "# save_tokenized_data(validation_tokenized_article, 'validation_tokenized_article.pkl')\n",
    "# save_tokenized_data(validation_tokenized_highlight, 'validation_tokenized_highlight.pkl')\n",
    "# save_tokenized_data(test_tokenized_article, 'test_tokenized_article.pkl')\n",
    "# save_tokenized_data(test_tokenized_highlight, 'test_tokenized_highlight.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a63598-c386-4c8c-add0-d31245f80d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Does the tokenizer work I wonder?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07520d-e8b5-4079-8d7f-6c61e86fd605",
   "metadata": {},
   "source": [
    "It does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e783064-9b85-4382-8aa2-44513c3d7385",
   "metadata": {},
   "source": [
    "#### Combine validation and test sets and resplit to create train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628c6db-2ac8-4310-a621-a4e27dc71dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets (concatenate validation and test sets)\n",
    "tokenized_articles_ids = validation_tokenized_article_ids + test_tokenized_article_ids\n",
    "tokenized_articles_masks = validation_tokenized_article_masks + test_tokenized_article_masks\n",
    "tokenized_highlights_ids = validation_tokenized_highlight_ids + test_tokenized_highlight_ids\n",
    "tokenized_highlights_masks = validation_tokenized_highlight_masks + test_tokenized_highlight_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fa964-5466-4f05-aded-3f8e3abdd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to flatten both input_ids and attention_masks\n",
    "def flatten_batches(input_batches, mask_batches):\n",
    "    flat_input_ids = []\n",
    "    flat_attention_masks = []\n",
    "    \n",
    "    for input_batch, mask_batch in tqdm(zip(input_batches, mask_batches), total=len(input_batches), desc=\"Flattening batches\"):\n",
    "        for input_item, mask_item in zip(input_batch, mask_batch):\n",
    "            flat_input_ids.append(input_item)\n",
    "            flat_attention_masks.append(mask_item)\n",
    "    \n",
    "    return flat_input_ids, flat_attention_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53198a0d-38e7-4121-9911-a9382e6bb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def flatten_batches_in_chunks(input_batches, mask_batches, chunk_size=1000):\n",
    "    flat_input_ids = []\n",
    "    flat_attention_masks = []\n",
    "\n",
    "    for i in range(0, len(input_batches), chunk_size):\n",
    "        # Process in smaller chunks to avoid high memory usage\n",
    "        chunk_input_batches = input_batches[i:i + chunk_size]\n",
    "        chunk_mask_batches = mask_batches[i:i + chunk_size]\n",
    "\n",
    "        # Flatten the current chunk\n",
    "        for input_batch, mask_batch in tqdm(zip(chunk_input_batches, chunk_mask_batches), total=len(input_batches), desc=\"Flattening batches\"):\n",
    "            for input_item, mask_item in zip(input_batch, mask_batch):\n",
    "                flat_input_ids.append(input_item)\n",
    "                flat_attention_masks.append(mask_item)\n",
    "\n",
    "        # Optionally save intermediate results to disk to reduce memory usage\n",
    "        # torch.save((flat_input_ids, flat_attention_masks), f\"flattened_chunk_{i//chunk_size}.pt\")\n",
    "\n",
    "    return flat_input_ids, flat_attention_masks\n",
    "\n",
    "# Flatten articles and highlights in smaller chunks\n",
    "flattened_article_ids, flattened_article_masks = flatten_batches_in_chunks(tokenized_articles_ids, tokenized_articles_masks, chunk_size=1000)\n",
    "flattened_highlight_ids, flattened_highlight_masks = flatten_batches_in_chunks(tokenized_highlights_ids, tokenized_highlights_masks, chunk_size=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c536cb1-eb0d-4b37-9e57-b334f4acc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Recreate train:val:test splits (splitting both input_ids and attention_masks)\n",
    "train_articles, temp_articles, train_highlights, temp_highlights = train_test_split(\n",
    "    flattened_article_ids, flattened_highlight_ids, test_size=0.3, random_state=10)\n",
    "\n",
    "train_article_masks, temp_article_masks, train_highlight_masks, temp_highlight_masks = train_test_split(\n",
    "    flattened_article_masks, flattened_highlight_masks, test_size=0.3, random_state=10)\n",
    "\n",
    "# Splitting again to give the test and val split\n",
    "validation_articles, test_articles, validation_highlights, test_highlights = train_test_split(\n",
    "    temp_articles, temp_highlights, test_size=0.5, random_state=10)\n",
    "\n",
    "validation_article_masks, test_article_masks, validation_highlight_masks, test_highlight_masks = train_test_split(\n",
    "    temp_article_masks, temp_highlight_masks, test_size=0.5, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e897ab-2ce9-4f9f-bb99-ca981493c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Articles: {len(train_articles)} |  Train highlights: {len(train_highlights)}\")\n",
    "print(f\"Val Articles: {len(validation_articles)} |  Val highlights: {len(validation_highlights)}\")\n",
    "print(f\"Test Articles: {len(test_articles)} |  Test highlights: {len(test_highlights)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac865ab2-577c-4aa1-98ce-09bd20129167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding Sanity Check\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        train_articles[1], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "\n",
    "print('\\nDECODED ARTICLE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ba6e5-39c6-45f1-b534-91664a58d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check max length of inputs \n",
    "max_length = max(len(x) for x in train_articles)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bdb5a-bf42-475e-a285-567422fe5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(train_articles[4]))\n",
    "print(\"\\n\\n\", 200*'-')\n",
    "print(tokenizer.decode(train_highlights[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f06cf5-151c-44be-ae66-8374fa78ef92",
   "metadata": {},
   "source": [
    "#### Test out pre-trained BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19435e23-89df-454a-b273-7032ac660255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model out on a few examples\n",
    "\n",
    "indexes = [1, 50, 120]\n",
    "\n",
    "\n",
    "# Iterate over the selected examples\n",
    "for i, index in enumerate(indexes):\n",
    "    # Extract the pre-tokenized article and highlight\n",
    "    article = train_articles[index]  # This is a tensor directly\n",
    "    highlight = train_highlights[index]  # Also a tensor\n",
    "\n",
    "    # Since 'article' is already a tensor, pass it directly to the model\n",
    "    inputs = article.unsqueeze(0)  # Add a batch dimension since the model expects a batch of inputs\n",
    "\n",
    "    # Generate a summary using the pre-tokenized input_ids\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode and print the results\n",
    "    print(dash_line)\n",
    "    \n",
    "    # Decode the tokenized article\n",
    "    decoded_input = tokenizer.decode(article.tolist(), skip_special_tokens=True)\n",
    "    print(f'INPUT PROMPT:\\n{decoded_input}')\n",
    "    \n",
    "    print(dash_line)\n",
    "    \n",
    "    # Decode the human-provided highlight (assuming it's already tokenized)\n",
    "    decoded_highlight = tokenizer.decode(highlight.tolist(), skip_special_tokens=True)\n",
    "    print(f'Baseline Human Highlights:\\n{decoded_highlight}')\n",
    "    \n",
    "    print(dash_line)\n",
    "    \n",
    "    # Decode the model's generated summary\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f'Base Model Generation:\\n{decoded_output}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740b8a2-0fec-4fda-b490-0f1b1d5efc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff907923-067d-45d2-a16a-aa6e4006e977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
